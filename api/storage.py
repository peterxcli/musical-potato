import base64
import hashlib
import shutil
import sys
from pathlib import Path
from typing import List

import schemas
from config import settings
from fastapi import Response, UploadFile, HTTPException, status
from loguru import logger
import aiofiles
import asyncio
import os


class Storage:
    def __init__(self, is_test: bool):
        logger.warning(f"NUM_DISKS: {settings.NUM_DISKS}")
        self.block_path: List[Path] = [
            Path("/tmp") / f"{settings.FOLDER_PREFIX}-{i}-test"
            if is_test
            else Path("/var/raid") / f"{settings.FOLDER_PREFIX}-{i}"
            for i in range(settings.NUM_DISKS)
        ]
        self.__create_block()

    def __create_block(self):
        # remove all block folders
        # if os.path.exists("/tmp"):
        #     shutil.rmtree("/tmp")
        # if os.path.exists("/var/raid"):
        #     shutil.rmtree("/var/raid")
        for path in self.block_path:
            logger.warning(f"Creating folder: {path}")
            path.mkdir(parents=True, exist_ok=True)

    # TODO: this code is generated by gpt, please check it
    async def read_block(self, block_file):
        async with aiofiles.open(block_file, 'rb') as f:
            return await f.read()

    # TODO: this code is generated by gpt, please check it
    async def write_block(self, block_file, content):
        async with aiofiles.open(block_file, 'wb') as f:
            await f.write(content)

    async def check_file(self, filename: str) -> bool:
        for block_path in self.block_path:
            block_file = Path.joinpath(block_path, filename)
            if block_file.exists():
                return True
        return False

    async def create_file(self, file: UploadFile) -> schemas.File:
        import math
        content = await file.read()
        checksum = hashlib.md5(content).hexdigest()

        # check if file already exists
        if await self.check_file(file.filename):
            raise HTTPException(
                status_code=status.HTTP_409_CONFLICT, detail="File already exists"
            )

        # calculate block size
        L = len(content)
        if L > settings.MAX_SIZE:
            raise HTTPException(
                status_code=status.HTTP_413_REQUEST_ENTITY_TOO_LARGE, detail="File too large"
            )
        N = settings.NUM_DISKS - 1
        block_size = math.floor(L / N)
        blocks = []
        cur_offset = 0
        for i in range(N):
            if i < L % N:
                blocks.append(content[cur_offset:cur_offset + block_size + 1])
                cur_offset += block_size + 1
            else:
                blocks.append(content[cur_offset:cur_offset + block_size])
                cur_offset += block_size

        # make sure all blocks are equally sized
        for i in range(len(blocks)):
            if len(blocks[i]) < block_size + 1:
                blocks[i] += b'\x00' * (block_size + 1 - len(blocks[i]))

        # write to each storage device
        for block, block_path in zip(blocks, self.block_path):
            await self.write_block(Path.joinpath(block_path, file.filename), block)

        # compute and write the parity block
        parity = bytearray(blocks[0])
        for i in range(1, len(blocks)):
            for j in range(len(parity)):
                parity[j] ^= blocks[i][j]

        await self.write_block(Path.joinpath(self.block_path[-1], file.filename), bytes(parity))

        return schemas.File(
            name=file.filename,
            size=len(content),
            checksum=checksum,
            content=base64.b64encode(content),
            content_type=file.content_type,
        )

    async def retrieve_file(self, filename: str) -> bytes:
        # check if file exists
        if not await self.check_file(filename):
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND, detail="File not found"
            )

        blocks = [await self.read_block(block_path / filename) for block_path in self.block_path[:-1]]
        content = bytearray()
        for i, block in enumerate(blocks):
            content += block.strip(b'\x00')
        return bytes(content)

    async def delete_file(self, filename: str) -> None:
        if not await self.check_file(filename):
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND, detail="File not found"
            )

        for block_path in self.block_path:
            try:
                os.remove(block_path / filename)
            except FileNotFoundError:
                pass

    async def update_file(self, file) -> schemas.File:
        # check if file exists
        if not await self.check_file(file.filename):
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND, detail="File not found"
            )

        await self.delete_file(file.filename)
        return await self.create_file(file)

    async def fix_block(self, block_id: int) -> None:
        # obtain all filenames in the affected block
        filename = set()
        healthy_block_path = []
        for block_path in self.block_path:
            if not block_path.exists():
                continue
            healthy_block_path.append(block_path)
            filenames = [f.name for f in (block_path).glob('*')]
            filename.update(filenames)
        filenames = list(filename)

        for filename in filenames:
            # read all other blocks and compute the XOR parity
            blocks = [await self.read_block(block_path / filename)
                      for i, block_path in enumerate(self.block_path) if i != block_id and (block_path / filename).exists()]

            # if there's no block to compute parity from, continue with the next file
            if not blocks:
                continue
            parity = bytearray(blocks[0])
            for i in range(1, len(blocks)):
                for j in range(len(parity)):
                    parity[j] ^= blocks[i][j]

            # write the computed parity to the missing block
            self.block_path[block_id].mkdir(parents=True, exist_ok=True)
            await self.write_block(self.block_path[block_id] / filename, bytes(parity))


storage: Storage = Storage(is_test="pytest" in sys.modules)
